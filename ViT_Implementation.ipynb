{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Simple ViT using Pytorch [20 marks]\n",
    "\n",
    "In this question, you will implement a vision transformer based image classification model using pytorch. \n",
    "Implement a basic version of vision transformer (https://arxiv.org/pdf/2010.11929.pdf), that first divides an image into patches and then passes them through a set of multihead self attention modules to perform classification. Please check out the details in the paper. Note that classification happens from the CLS token of the final transformer layer.\n",
    "\n",
    "[Experiment 1] Train this model on the CIFAR-10 dataset for 10-class classification. Keep the number of attention heads to be 4 for all the experiments. [6 points]\n",
    "\n",
    "[Experiment 2] Try out different patch sizes (like 4x4, 8x8, 16x16). You can divide the image into both overlapping and non-overlapping patches. [4 points]\n",
    "\n",
    "[Experiment 3] How does model performance change if you vary the number of attention heads? [4 points]\n",
    "\n",
    "[Experiment 4] Perform classification by using the CLS token from different layers of the model. [6 points]\n",
    "Create a detailed report of all the experiments and analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('LogitAdjustmentLongTail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49598912cb7efc65e0007e347a7051cc5ac4c91b95dad2ffbc631da6724968c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
